{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb4d731bcb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0])\n",
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]])\n",
      "tensor([[-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]])\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "lookup_tensor1 = torch.tensor([word_to_ix[\"world\"]], dtype=torch.long)\n",
    "print(lookup_tensor)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "hello_embed1 = embeds(lookup_tensor1)\n",
    "print(hello_embed)\n",
    "# print(hello_embed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n",
      "97\n",
      "tensor([[-0.3978, -1.9291,  0.9264,  0.9134, -0.7902, -1.2617,  1.9147,\n",
      "         -1.8613,  0.4739,  0.9988]])\n",
      "tensor([[-1.2617,  1.9147, -1.8613,  0.4739,  0.9988,  0.4841, -0.7030,\n",
      "         -0.8268,  0.1012,  0.1952]])\n",
      "tensor([[ 0.4842, -0.7030, -0.8269,  0.1013,  0.1953, -0.9773,  0.8748,\n",
      "          0.9873,  0.2505, -0.7930]])\n",
      "tensor([[-0.9773,  0.8749,  0.9872,  0.2504, -0.7930, -0.8044, -1.0371,\n",
      "         -1.0669, -0.2085, -0.2155]])\n",
      "tensor([[-0.8044, -1.0372, -1.0669, -0.2085, -0.2157,  1.9907, -0.9247,\n",
      "         -0.9301,  1.4301,  0.4208]])\n",
      "tensor([[ 1.9907, -0.9248, -0.9301,  1.4301,  0.4208, -1.3034,  0.5190,\n",
      "         -0.5464, -0.8024,  0.0186]])\n",
      "tensor([[-1.3033,  0.5190, -0.5463, -0.8023,  0.0185, -0.5458,  1.1528,\n",
      "         -0.3997,  0.9738,  1.5453]])\n",
      "tensor([[-0.5458,  1.1529, -0.3997,  0.9738,  1.5453,  1.2563,  0.5000,\n",
      "          0.0402,  0.4647, -3.3793]])\n",
      "tensor([[ 1.2563,  0.4999,  0.0402,  0.4647, -3.3791,  0.8657,  0.2444,\n",
      "         -0.6629,  0.8073,  0.4391]])\n",
      "tensor([[ 0.8657,  0.2443, -0.6629,  0.8073,  0.4392, -0.5831, -0.7821,\n",
      "         -1.4237,  1.6091, -0.0328]])\n",
      "tensor([[-0.5832, -0.7822, -1.4237,  1.6090, -0.0329,  0.1945, -0.9418,\n",
      "         -0.9965,  0.8073,  1.1739]])\n",
      "tensor([[ 0.1945, -0.9418, -0.9964,  0.8074,  1.1737,  1.9906, -0.9249,\n",
      "         -0.9300,  1.4301,  0.4208]])\n",
      "tensor([[ 1.9904, -0.9249, -0.9299,  1.4302,  0.4208,  0.2186, -0.5743,\n",
      "          1.4571,  1.7710, -2.0173]])\n",
      "tensor([[ 0.2185, -0.5743,  1.4572,  1.7710, -2.0173, -1.3889,  1.7241,\n",
      "         -2.3648, -0.9295,  0.2936]])\n",
      "tensor([[-1.3889,  1.7240, -2.3647, -0.9294,  0.2937,  2.0154,  0.2152,\n",
      "         -0.5242, -1.8034, -1.3083]])\n",
      "tensor([[ 2.0154,  0.2153, -0.5242, -1.8033, -1.3083, -1.0655, -0.3865,\n",
      "          0.3782, -0.2076,  0.6936]])\n",
      "tensor([[-1.0654, -0.3865,  0.3782, -0.2077,  0.6936,  2.4142,  1.0206,\n",
      "         -0.4405, -1.7342, -1.0257]])\n",
      "tensor([[ 2.4142,  1.0206, -0.4404, -1.7341, -1.0257,  0.6789, -0.4302,\n",
      "         -0.3849,  0.8565,  0.3969]])\n",
      "tensor([[ 0.6790, -0.4301, -0.3849,  0.8564,  0.3969,  0.5051, -0.1970,\n",
      "         -0.0334,  0.7193,  1.0644]])\n",
      "tensor([[ 0.5050, -0.1971, -0.0334,  0.7193,  1.0644, -0.7981, -0.1316,\n",
      "         -0.7984,  0.3357,  0.2753]])\n",
      "tensor([[-0.7980, -0.1315, -0.7985,  0.3357,  0.2753, -0.8743,  0.2053,\n",
      "          0.3051,  0.5357, -0.4312]])\n",
      "tensor([[-0.8742,  0.2053,  0.3052,  0.5357, -0.4313,  0.2477, -0.3867,\n",
      "         -0.2004, -0.8375, -1.5519]])\n",
      "tensor([[ 0.2477, -0.3869, -0.2004, -0.8376, -1.5518,  1.7163,  0.1991,\n",
      "          0.0457,  0.1530, -0.4757]])\n",
      "tensor([[ 1.7162,  0.1990,  0.0457,  0.1530, -0.4756,  0.5514, -1.5474,\n",
      "          0.7575, -0.4068, -0.1277]])\n",
      "tensor([[ 0.5513, -1.5473,  0.7575, -0.4067, -0.1278,  0.3880,  0.5149,\n",
      "         -1.8475, -2.9167, -0.5673]])\n",
      "tensor([[ 0.3880,  0.5150, -1.8474, -2.9168, -0.5673, -0.4728, -0.7531,\n",
      "         -0.4319,  0.6693,  0.6505]])\n",
      "tensor([[-0.4728, -0.7531, -0.4319,  0.6692,  0.6505, -1.1064,  0.1571,\n",
      "         -0.5976, -0.8839,  0.6077]])\n",
      "tensor([[-1.1064,  0.1570, -0.5976, -0.8839,  0.6077,  0.5231,  1.2236,\n",
      "         -0.4035, -0.9591, -0.0052]])\n",
      "tensor([[ 0.5231,  1.2236, -0.4036, -0.9590, -0.0053, -2.3381, -0.8291,\n",
      "         -0.1056, -1.1797, -0.0908]])\n",
      "tensor([[-2.3381, -0.8291, -0.1056, -1.1797, -0.0906, -0.0576,  1.1982,\n",
      "         -2.2833, -1.0130, -0.8879]])\n",
      "tensor([[-0.0576,  1.1982, -2.2832, -1.0129, -0.8879,  0.1437,  0.2031,\n",
      "          1.0540,  0.1317,  1.4023]])\n",
      "tensor([[ 0.1437,  0.2030,  1.0540,  0.1316,  1.4023, -0.5008,  0.1716,\n",
      "         -0.1600, -0.5047, -1.4746]])\n",
      "tensor([[-0.5009,  0.1717, -0.1601, -0.5046, -1.4745,  0.9196, -0.5484,\n",
      "         -0.3472,  0.4730, -0.4286]])\n",
      "tensor([[ 0.9195, -0.5482, -0.3472,  0.4729, -0.4286, -0.9640,  0.1415,\n",
      "         -0.1637, -0.3582, -0.0594]])\n",
      "tensor([[-0.9640,  0.1416, -0.1637, -0.3581, -0.0594, -0.4886,  1.3694,\n",
      "          0.4539,  1.1753, -0.3421]])\n",
      "tensor([[-0.4886,  1.3693,  0.4539,  1.1753, -0.3421,  0.5631, -0.5873,\n",
      "         -2.0619,  0.4305,  0.3377]])\n",
      "tensor([[ 0.5631, -0.5873, -2.0619,  0.4304,  0.3375,  1.9904, -0.9250,\n",
      "         -0.9298,  1.4304,  0.4208]])\n",
      "tensor([[ 1.9904, -0.9250, -0.9297,  1.4304,  0.4208,  0.7225,  0.1526,\n",
      "          0.1450, -2.3442, -0.4619]])\n",
      "tensor([[ 0.7223,  0.1526,  0.1450, -2.3442, -0.4620, -1.4782, -1.1334,\n",
      "         -0.1010,  0.3434, -1.0703]])\n",
      "tensor([[-1.4782, -1.1334, -0.1010,  0.3433, -1.0703, -1.0373,  1.5748,\n",
      "         -0.6298,  2.4070,  0.2786]])\n",
      "tensor([[-1.0374,  1.5747, -0.6298,  2.4070,  0.2786,  0.5631, -0.5875,\n",
      "         -2.0619,  0.4304,  0.3375]])\n",
      "tensor([[ 0.5630, -0.5873, -2.0619,  0.4304,  0.3376, -0.8336, -1.1929,\n",
      "         -2.3065,  0.6037,  0.3151]])\n",
      "tensor([[-0.8337, -1.1928, -2.3065,  0.6037,  0.3151, -2.6189,  2.2544,\n",
      "         -1.1085, -1.8874,  0.7262]])\n",
      "tensor([[-2.6189,  2.2543, -1.1084, -1.8875,  0.7262,  0.5232,  1.2235,\n",
      "         -0.4035, -0.9590, -0.0052]])\n",
      "tensor([[ 0.5231,  1.2234, -0.4035, -0.9590, -0.0054,  1.9902, -0.9249,\n",
      "         -0.9296,  1.4303,  0.4208]])\n",
      "tensor([[ 1.9903, -0.9248, -0.9297,  1.4304,  0.4208,  0.1927,  0.3429,\n",
      "          1.4161,  0.4738,  0.0827]])\n",
      "tensor([[ 0.1927,  0.3430,  1.4161,  0.4737,  0.0827,  0.7879,  1.3686,\n",
      "         -0.8507,  0.5126,  0.3331]])\n",
      "tensor([[ 0.7879,  1.3685, -0.8506,  0.5126,  0.3331, -0.3761, -2.4107,\n",
      "         -1.2778, -0.0629, -1.2308]])\n",
      "tensor([[-0.3762, -2.4107, -1.2779, -0.0628, -1.2307, -0.0789, -0.3891,\n",
      "         -0.0796,  0.7605, -1.0025]])\n",
      "tensor([[-0.0790, -0.3893, -0.0796,  0.7604, -1.0026,  1.6604,  0.2717,\n",
      "         -0.8087,  0.1267, -0.7448]])\n",
      "tensor([[ 1.6604,  0.2716, -0.8087,  0.1266, -0.7447, -0.3686,  0.3682,\n",
      "          0.4850,  0.1988,  0.5441]])\n",
      "tensor([[-0.3688,  0.3683,  0.4851,  0.1987,  0.5441,  0.4235,  0.5730,\n",
      "         -1.7962,  1.2470,  1.2738]])\n",
      "tensor([[ 0.4236,  0.5730, -1.7961,  1.2469,  1.2739,  0.8658,  0.2444,\n",
      "         -0.6629,  0.8074,  0.4391]])\n",
      "tensor([[ 0.8659,  0.2445, -0.6629,  0.8075,  0.4391,  0.2981,  0.1099,\n",
      "         -0.6463, -0.2106, -0.0075]])\n",
      "tensor([[ 0.2981,  0.1098, -0.6464, -0.2106, -0.0075,  0.2804,  1.7460,\n",
      "          1.8550, -0.7064,  2.5571]])\n",
      "tensor([[ 0.2805,  1.7462,  1.8550, -0.7064,  2.5572,  1.1712,  1.7674,\n",
      "         -0.0954,  0.0612, -0.6177]])\n",
      "tensor([[ 1.1712,  1.7675, -0.0954,  0.0611, -0.6177,  1.6734,  0.0103,\n",
      "          0.9837,  0.8793, -1.4504]])\n",
      "tensor([[ 1.6735,  0.0104,  0.9838,  0.8793, -1.4503,  1.7986,  0.1018,\n",
      "          0.3400, -0.6447, -0.2870]])\n",
      "tensor([[ 1.7986,  0.1019,  0.3399, -0.6447, -0.2871, -1.1992, -0.0474,\n",
      "         -2.0030, -0.4914, -1.5458]])\n",
      "tensor([[-1.1992, -0.0474, -2.0029, -0.4913, -1.5457, -1.0920, -0.7129,\n",
      "         -0.0639,  1.0757, -0.5536]])\n",
      "tensor([[-1.0920, -0.7129, -0.0638,  1.0756, -0.5536, -2.4919,  0.2423,\n",
      "          0.2883,  0.1032,  1.1004]])\n",
      "tensor([[-2.4918,  0.2422,  0.2883,  0.1033,  1.1004,  0.1573,  1.2540,\n",
      "          1.3275, -0.4954, -1.9804]])\n",
      "tensor([[ 0.1574,  1.2540,  1.3275, -0.4955, -1.9804,  0.5213, -0.4531,\n",
      "         -0.1260, -0.5882,  2.1189]])\n",
      "tensor([[ 0.5213, -0.4531, -0.1261, -0.5881,  2.1190,  1.3851, -0.8138,\n",
      "         -0.9276,  1.1120,  0.6155]])\n",
      "tensor([[ 1.3851, -0.8139, -0.9276,  1.1120,  0.6155, -0.9398,  0.3861,\n",
      "          1.0473, -0.7327, -0.9168]])\n",
      "tensor([[-0.9398,  0.3862,  1.0474, -0.7328, -0.9168,  2.2952,  0.6749,\n",
      "          1.7133, -1.7943, -1.5208]])\n",
      "tensor([[ 2.2952,  0.6749,  1.7134, -1.7943, -1.5210,  0.6867,  0.4209,\n",
      "         -1.0214,  0.9886,  0.7806]])\n",
      "tensor([[ 0.6866,  0.4210, -1.0215,  0.9886,  0.7805,  1.9902, -0.9248,\n",
      "         -0.9296,  1.4304,  0.4207]])\n",
      "tensor([[ 1.9901, -0.9248, -0.9295,  1.4306,  0.4208,  0.2185, -0.5744,\n",
      "          1.4571,  1.7708, -2.0173]])\n",
      "tensor([[ 0.2185, -0.5743,  1.4571,  1.7707, -2.0172, -3.2686,  0.4750,\n",
      "         -2.1142, -1.5002,  0.2693]])\n",
      "tensor([[-3.2686,  0.4751, -2.1143, -1.5001,  0.2693,  0.2468,  1.1843,\n",
      "          0.7626,  0.4415,  1.1651]])\n",
      "tensor([[ 0.2468,  1.1843,  0.7625,  0.4415,  1.1651,  0.3453, -0.5506,\n",
      "         -0.9604,  0.2769, -0.6801]])\n",
      "tensor([[ 0.3453, -0.5506, -0.9604,  0.2769, -0.6802,  0.2648, -1.6050,\n",
      "         -0.1064,  0.2466,  0.6125]])\n",
      "tensor([[ 0.2650, -1.6050, -0.1065,  0.2465,  0.6125,  1.1349,  0.8658,\n",
      "          0.6334, -0.5392,  0.0182]])\n",
      "tensor([[ 1.1349,  0.8656,  0.6333, -0.5393,  0.0183, -1.8821, -0.7765,\n",
      "          2.0242, -0.0865,  2.3571]])\n",
      "tensor([[-1.8821, -0.7765,  2.0243, -0.0866,  2.3571,  0.7705, -1.0739,\n",
      "         -0.2015, -0.5603, -0.6240]])\n",
      "tensor([[ 0.7705, -1.0739, -0.2016, -0.5602, -0.6241, -0.3416, -0.3003,\n",
      "         -1.0483, -0.4709,  0.2911]])\n",
      "tensor([[-0.3415, -0.3003, -1.0484, -0.4709,  0.2912,  0.5231,  1.2234,\n",
      "         -0.4035, -0.9590, -0.0054]])\n",
      "tensor([[ 0.5231,  1.2234, -0.4035, -0.9590, -0.0054, -0.3417,  0.9473,\n",
      "          0.6223, -0.4481, -0.2856]])\n",
      "tensor([[-0.3417,  0.9475,  0.6224, -0.4481, -0.2857,  1.1033,  0.5368,\n",
      "          0.9805, -1.1940, -0.4919]])\n",
      "tensor([[ 1.1031,  0.5367,  0.9805, -1.1941, -0.4920,  0.1938, -2.5832,\n",
      "          0.8539, -2.1021, -0.6200]])\n",
      "tensor([[ 0.1940, -2.5831,  0.8540, -2.1020, -0.6202,  1.5392, -0.8696,\n",
      "         -3.3312, -0.7479,  1.1173]])\n",
      "tensor([[ 1.5391, -0.8698, -3.3312, -0.7478,  1.1173, -0.2219,  0.3421,\n",
      "          1.1093, -0.5276,  0.0849]])\n",
      "tensor([[-0.2220,  0.3422,  1.1093, -0.5275,  0.0848, -1.0921, -0.7129,\n",
      "         -0.0638,  1.0757, -0.5535]])\n",
      "tensor([[-1.0920, -0.7129, -0.0637,  1.0756, -0.5533,  1.3245, -0.0705,\n",
      "          0.3470, -0.6537,  1.5586]])\n",
      "tensor([[ 1.3246, -0.0705,  0.3470, -0.6537,  1.5585,  1.5391, -0.8698,\n",
      "         -3.3312, -0.7478,  1.1173]])\n",
      "tensor([[ 1.5390, -0.8699, -3.3311, -0.7478,  1.1174,  0.1522, -0.2980,\n",
      "         -0.1314,  1.5365,  0.9193]])\n",
      "tensor([[ 0.1522, -0.2980, -0.1313,  1.5367,  0.9194,  1.1079,  0.1857,\n",
      "         -0.2764, -0.6116,  0.8160]])\n",
      "tensor([[ 1.1079,  0.1857, -0.2764, -0.6116,  0.8160, -1.1042, -0.6994,\n",
      "          0.2352,  1.9142,  1.8364]])\n",
      "tensor([[-1.1042, -0.6994,  0.2352,  1.9142,  1.8365, -0.0077,  2.6158,\n",
      "         -0.0639,  0.2609, -0.3608]])\n",
      "tensor([[-0.0076,  2.6158, -0.0639,  0.2609, -0.3608,  0.7223,  0.1526,\n",
      "          0.1449, -2.3442, -0.4619]])\n",
      "tensor([[ 0.7223,  0.1526,  0.1449, -2.3442, -0.4619,  0.3909,  0.3872,\n",
      "          2.6415, -0.9624, -0.2076]])\n",
      "tensor([[ 0.3909,  0.3871,  2.6415, -0.9624, -0.2076, -0.2693,  1.3120,\n",
      "         -1.5563, -1.0757, -0.8752]])\n",
      "tensor([[-0.2693,  1.3120, -1.5563, -1.0758, -0.8752,  0.4195,  2.2524,\n",
      "         -0.0802, -1.3489, -0.8263]])\n",
      "tensor([[ 0.4196,  2.2524, -0.0801, -1.3490, -0.8262, -1.1796, -1.4423,\n",
      "         -0.7071,  0.7521, -0.0192]])\n",
      "tensor([[-1.1795, -1.4423, -0.7073,  0.7520, -0.0191, -0.2022, -0.2297,\n",
      "          0.0013,  0.1444,  0.7772]])\n",
      "tensor([[-0.2022, -0.2297,  0.0013,  0.1444,  0.7772, -0.3872,  0.5477,\n",
      "         -0.8026,  0.3036, -2.0311]])\n",
      "tensor([[-0.3872,  0.5476, -0.8027,  0.3036, -2.0311,  0.5514, -1.5473,\n",
      "          0.7575, -0.4066, -0.1278]])\n",
      "tensor([[ 0.5515, -1.5475,  0.7576, -0.4067, -0.1278, -0.4327, -0.1150,\n",
      "          1.7964, -0.4774, -2.7188]])\n",
      "tensor([[-0.4328, -0.1148,  1.7965, -0.4774, -2.7188,  0.9732, -0.6191,\n",
      "         -0.6363, -0.4242, -2.0272]])\n",
      "tensor([[ 0.9732, -0.6191, -0.6364, -0.4243, -2.0272,  3.3212, -0.4021,\n",
      "         -0.3030, -1.7618,  0.6348]])\n",
      "tensor([[ 3.3213, -0.4023, -0.3030, -1.7619,  0.6349,  0.3454, -0.5506,\n",
      "         -0.9605,  0.2769, -0.6801]])\n",
      "tensor([[ 0.3453, -0.5507, -0.9605,  0.2769, -0.6801,  1.1423,  0.3055,\n",
      "         -0.5789,  0.5644, -0.8773]])\n",
      "tensor([[ 1.1423,  0.3056, -0.5789,  0.5644, -0.8772, -0.1733,  0.7282,\n",
      "          0.0571, -0.7092, -0.5262]])\n",
      "tensor([[-0.1733,  0.7283,  0.0571, -0.7091, -0.5263, -0.5458,  1.1528,\n",
      "         -0.3997,  0.9738,  1.5453]])\n",
      "tensor([[-0.5458,  1.1527, -0.3997,  0.9738,  1.5453, -1.1802, -0.4610,\n",
      "         -0.5601,  0.3956, -0.9823]])\n",
      "tensor([[-1.1803, -0.4611, -0.5601,  0.3955, -0.9825,  1.9903, -0.9249,\n",
      "         -0.9294,  1.4306,  0.4208]])\n",
      "tensor([[ 1.9902, -0.9249, -0.9293,  1.4306,  0.4207,  1.3015, -0.6293,\n",
      "         -0.0683,  0.6945, -0.2890]])\n",
      "tensor([[ 1.3014, -0.6294, -0.0684,  0.6944, -0.2891, -0.5422, -2.4593,\n",
      "         -0.9502, -0.3096,  1.6633]])\n",
      "tensor([[-0.5422, -2.4593, -0.9504, -0.3096,  1.6632,  3.3212, -0.4022,\n",
      "         -0.3031, -1.7619,  0.6348]])\n",
      "tensor([[ 3.3214, -0.4023, -0.3032, -1.7619,  0.6348,  0.3452, -0.5508,\n",
      "         -0.9604,  0.2768, -0.6800]])\n",
      "tensor([[ 0.3451, -0.5507, -0.9604,  0.2767, -0.6799,  1.3264,  0.8547,\n",
      "         -0.2805,  0.7000, -1.4344]])\n",
      "tensor([[ 1.3263,  0.8546, -0.2806,  0.7000, -1.4342, -2.2049, -1.4975,\n",
      "         -0.9023,  0.8008, -0.6619]])\n",
      "[tensor([ 517.5781])]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 5\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        print(embeds)\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "print(len(vocab))\n",
    "for epoch in range(1):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        #print(context_idxs, target)\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a variable)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = \"this is a test and it's working perfect. it would really nice and is working\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voc = set(sent)\n",
    "voc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_to_idx = {word:i for word, i in enumerate(voc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'it',\n",
       " 1: 'would',\n",
       " 2: 'this',\n",
       " 3: 'working',\n",
       " 4: \"it's\",\n",
       " 5: 'and',\n",
       " 6: 'is',\n",
       " 7: 'perfect.',\n",
       " 8: 'test',\n",
       " 9: 'really',\n",
       " 10: 'a',\n",
       " 11: 'nice'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
